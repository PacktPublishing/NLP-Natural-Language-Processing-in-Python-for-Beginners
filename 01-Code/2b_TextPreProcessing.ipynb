{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 'I always uh do the main um processing, I mean, the uh um data-processing.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "always\n",
      "uh\n",
      "do\n",
      "the\n",
      "main\n",
      "um\n",
      "processing\n",
      ",\n",
      "I\n",
      "mean\n",
      ",\n",
      "the\n",
      "uh\n",
      "um\n",
      "data\n",
      "-\n",
      "processing\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in stats:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.K.\n",
      "has\n",
      "a\n",
      "reasonable\n",
      "population\n"
     ]
    }
   ],
   "source": [
    "doc2 = 'U.K. has a reasonable population'\n",
    "stats = nlp(doc2)\n",
    "for token in stats:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "always\n",
      "uh\n",
      "do\n",
      "the\n",
      "main\n",
      "um\n",
      "processing\n",
      "I\n",
      "mean\n",
      "the\n",
      "uh\n",
      "um\n",
      "data\n",
      "processing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in re.split('\\W+',doc):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U\n",
      "K\n",
      "has\n",
      "a\n",
      "reasonable\n",
      "population\n"
     ]
    }
   ],
   "source": [
    "for token in re.split('\\W+',doc2):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = list(nlp.vocab.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83431"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barnabas'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L[50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lathe'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L[60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t',\n",
       " 'en',\n",
       " '\\n',\n",
       " ' ',\n",
       " \"'\",\n",
       " \"''\",\n",
       " '\"',\n",
       " \"'Cause\",\n",
       " 'because',\n",
       " \"'cause\",\n",
       " 'use',\n",
       " \"'Xxxxx\",\n",
       " 'Cause',\n",
       " 'cause',\n",
       " 'C',\n",
       " 'Xxxxx',\n",
       " \"'Cos\",\n",
       " \"'cos\",\n",
       " 'Cos',\n",
       " \"'Xxx\",\n",
       " 'cos',\n",
       " 'Xxx',\n",
       " \"'Coz\",\n",
       " \"'coz\",\n",
       " 'Coz',\n",
       " 'coz',\n",
       " \"'Cuz\",\n",
       " \"'cuz\",\n",
       " 'Cuz',\n",
       " 'cuz',\n",
       " \"'S\",\n",
       " \"'s\",\n",
       " \"'X\",\n",
       " 'S',\n",
       " 's',\n",
       " \"'bout\",\n",
       " 'about',\n",
       " 'out',\n",
       " \"'xxxx\",\n",
       " 'bout',\n",
       " 'b',\n",
       " 'xxxx',\n",
       " 'c',\n",
       " \"'xxx\",\n",
       " 'xxx',\n",
       " \"'d\",\n",
       " \"'x\",\n",
       " 'd',\n",
       " 'x',\n",
       " \"'em\",\n",
       " 'them',\n",
       " \"'xx\",\n",
       " 'em',\n",
       " 'e',\n",
       " 'xx',\n",
       " \"'ll\",\n",
       " 'will',\n",
       " 'll',\n",
       " 'l',\n",
       " \"'nuff\",\n",
       " 'enough',\n",
       " 'uff',\n",
       " 'nuff',\n",
       " 'n',\n",
       " \"'re\",\n",
       " 'are',\n",
       " 're',\n",
       " 'r',\n",
       " '(*_*)',\n",
       " '(',\n",
       " '_*)',\n",
       " ')',\n",
       " '*',\n",
       " '(-8',\n",
       " '(-d',\n",
       " '-8',\n",
       " '-',\n",
       " '-d',\n",
       " '(-:',\n",
       " ':',\n",
       " '(-;',\n",
       " ';',\n",
       " '(-_-)',\n",
       " '_-)',\n",
       " '-_-',\n",
       " '(._.)',\n",
       " '_.)',\n",
       " '.',\n",
       " '(:',\n",
       " '(;',\n",
       " '(=',\n",
       " '=',\n",
       " '(>_<)',\n",
       " '_<)',\n",
       " '>',\n",
       " '<',\n",
       " '(^_^)',\n",
       " '_^)',\n",
       " '^_^',\n",
       " '^',\n",
       " '(o:',\n",
       " '(x:',\n",
       " 'o',\n",
       " '(¬_¬)',\n",
       " '_¬)',\n",
       " '¬_¬',\n",
       " '¬',\n",
       " '(ಠ_ಠ)',\n",
       " '_ಠ)',\n",
       " '(x_x)',\n",
       " 'ಠ_ಠ',\n",
       " 'ಠ',\n",
       " 'x_x',\n",
       " '(╯°□°）╯︵┻━┻',\n",
       " '┻━┻',\n",
       " '┻',\n",
       " '╯',\n",
       " '━',\n",
       " '°',\n",
       " '□',\n",
       " '）',\n",
       " '︵',\n",
       " ')-:',\n",
       " '):',\n",
       " '-__-',\n",
       " '__-',\n",
       " '._.',\n",
       " '0.0',\n",
       " '0',\n",
       " 'd.d',\n",
       " '0.o',\n",
       " 'd.x',\n",
       " '0_0',\n",
       " 'd_d',\n",
       " '0_o',\n",
       " 'd_x',\n",
       " '10',\n",
       " '1',\n",
       " 'dd',\n",
       " 'a.m.',\n",
       " 'a',\n",
       " '.m.',\n",
       " 'x.x.',\n",
       " '10a.m',\n",
       " 'a.m',\n",
       " 'ddx.x',\n",
       " '10a.m.',\n",
       " 'ddx.x.',\n",
       " 'am',\n",
       " 'p.m.',\n",
       " 'p',\n",
       " '10p.m',\n",
       " 'p.m',\n",
       " '10p.m.',\n",
       " 'pm',\n",
       " '11',\n",
       " '11a.m',\n",
       " '11a.m.',\n",
       " '11p.m',\n",
       " '11p.m.',\n",
       " '12',\n",
       " '12a.m',\n",
       " '12a.m.',\n",
       " '12p.m',\n",
       " '12p.m.',\n",
       " '1a.m',\n",
       " 'dx.x',\n",
       " '1a.m.',\n",
       " 'dx.x.',\n",
       " '1p.m',\n",
       " '1p.m.',\n",
       " '2',\n",
       " '2a.m',\n",
       " '2a.m.',\n",
       " '2p.m',\n",
       " '2p.m.',\n",
       " '3',\n",
       " '3a.m',\n",
       " '3a.m.',\n",
       " '3p.m',\n",
       " '3p.m.',\n",
       " '4',\n",
       " '4a.m',\n",
       " '4a.m.',\n",
       " '4p.m',\n",
       " '4p.m.',\n",
       " '5',\n",
       " '5a.m',\n",
       " '5a.m.',\n",
       " '5p.m',\n",
       " '5p.m.',\n",
       " '6',\n",
       " '6a.m',\n",
       " '6a.m.',\n",
       " '6p.m',\n",
       " '6p.m.',\n",
       " '7',\n",
       " '7a.m',\n",
       " '7a.m.',\n",
       " '7p.m',\n",
       " '7p.m.',\n",
       " '8)',\n",
       " '8',\n",
       " 'd)',\n",
       " '8-)',\n",
       " 'd-)',\n",
       " '8-',\n",
       " 'd-',\n",
       " '8-D',\n",
       " '8-d',\n",
       " 'd-X',\n",
       " 'D',\n",
       " '8D',\n",
       " '8d',\n",
       " 'dX',\n",
       " '8a.m',\n",
       " '8a.m.',\n",
       " '8p.m',\n",
       " '8p.m.',\n",
       " '9',\n",
       " '9a.m',\n",
       " '9a.m.',\n",
       " '9p.m',\n",
       " '9p.m.',\n",
       " \":'(\",\n",
       " \":')\",\n",
       " \":'-(\",\n",
       " \"'-(\",\n",
       " \":'-)\",\n",
       " \"'-)\",\n",
       " ':(',\n",
       " ':((',\n",
       " ':(((',\n",
       " '(((',\n",
       " ':()',\n",
       " ':)',\n",
       " ':))',\n",
       " ':)))',\n",
       " ')))',\n",
       " ':*',\n",
       " ':-(',\n",
       " ':-((',\n",
       " '-((',\n",
       " ':-(((',\n",
       " ':-)',\n",
       " ':-))',\n",
       " '-))',\n",
       " ':-)))',\n",
       " ':-*',\n",
       " ':-/',\n",
       " '-/',\n",
       " ':-0',\n",
       " ':-d',\n",
       " '-0',\n",
       " ':-3',\n",
       " '-3',\n",
       " ':->',\n",
       " ':-D',\n",
       " ':-X',\n",
       " '-D',\n",
       " '-X',\n",
       " ':-O',\n",
       " ':-o',\n",
       " '-O',\n",
       " '-o',\n",
       " ':-P',\n",
       " ':-p',\n",
       " '-P',\n",
       " '-p',\n",
       " ':-x',\n",
       " '-x',\n",
       " ':-]',\n",
       " ']',\n",
       " ':-|',\n",
       " '-|',\n",
       " ':-}',\n",
       " '}',\n",
       " ':/',\n",
       " '/',\n",
       " ':0',\n",
       " ':d',\n",
       " ':1',\n",
       " ':3',\n",
       " ':>',\n",
       " ':D',\n",
       " ':X',\n",
       " ':O',\n",
       " ':o',\n",
       " 'O',\n",
       " ':P',\n",
       " ':p',\n",
       " 'P',\n",
       " ':x',\n",
       " ':]',\n",
       " ':o)',\n",
       " ':x)',\n",
       " ':|',\n",
       " '|',\n",
       " ':}',\n",
       " ':’(',\n",
       " '’',\n",
       " ':’)',\n",
       " ':’-(',\n",
       " '’-(',\n",
       " ':’-)',\n",
       " '’-)',\n",
       " ';)',\n",
       " ';-)',\n",
       " ';-D',\n",
       " ';-d',\n",
       " ';-X',\n",
       " ';D',\n",
       " ';d',\n",
       " ';X',\n",
       " ';_;',\n",
       " '<.<',\n",
       " '</3',\n",
       " '</d',\n",
       " '/3',\n",
       " '/d',\n",
       " '<3',\n",
       " '<d',\n",
       " '<33',\n",
       " '<dd',\n",
       " '33',\n",
       " '<333',\n",
       " '333',\n",
       " '<ddd',\n",
       " 'ddd',\n",
       " '<space>',\n",
       " 'ce>',\n",
       " '<xxxx>',\n",
       " 'space',\n",
       " 'ace',\n",
       " '=(',\n",
       " '=)',\n",
       " '=/',\n",
       " '=3',\n",
       " '=d',\n",
       " '=D',\n",
       " '=X',\n",
       " '=[',\n",
       " '[',\n",
       " '=]',\n",
       " '=|',\n",
       " '>.<',\n",
       " '>.>',\n",
       " '>:(',\n",
       " '>:o',\n",
       " '>:x',\n",
       " '><(((*>',\n",
       " '(*>',\n",
       " '@_@',\n",
       " '@',\n",
       " 'Adm.',\n",
       " 'adm.',\n",
       " 'A',\n",
       " 'dm.',\n",
       " 'Xxx.',\n",
       " 'Adm',\n",
       " 'adm',\n",
       " 'Ai',\n",
       " 'ai',\n",
       " 'Xx',\n",
       " \"n't\",\n",
       " 'not',\n",
       " \"x'x\",\n",
       " 'nt',\n",
       " 'n’t',\n",
       " 'x’x',\n",
       " 'Ak.',\n",
       " 'Alaska',\n",
       " 'ak.',\n",
       " 'Xx.',\n",
       " 'Ak',\n",
       " 'ak',\n",
       " 'Ala.',\n",
       " 'Alabama',\n",
       " 'ala.',\n",
       " 'la.',\n",
       " 'Ala',\n",
       " 'ala',\n",
       " 'Apr.',\n",
       " 'April',\n",
       " 'apr.',\n",
       " 'pr.',\n",
       " 'Apr',\n",
       " 'apr',\n",
       " 'Are',\n",
       " 'Ariz.',\n",
       " 'Arizona',\n",
       " 'ariz.',\n",
       " 'iz.',\n",
       " 'Xxxx.',\n",
       " 'Ariz',\n",
       " 'ariz',\n",
       " 'riz',\n",
       " 'Xxxx',\n",
       " 'Ark.',\n",
       " 'Arkansas',\n",
       " 'ark.',\n",
       " 'rk.',\n",
       " 'Ark',\n",
       " 'ark',\n",
       " 'Aug.',\n",
       " 'August',\n",
       " 'aug.',\n",
       " 'ug.',\n",
       " 'Aug',\n",
       " 'aug',\n",
       " 'Bros.',\n",
       " 'bros.',\n",
       " 'B',\n",
       " 'os.',\n",
       " 'Bros',\n",
       " 'bros',\n",
       " 'ros',\n",
       " \"C'm\",\n",
       " 'come',\n",
       " \"c'm\",\n",
       " \"X'x\",\n",
       " 'on',\n",
       " 'C++',\n",
       " 'c++',\n",
       " 'X++',\n",
       " 'Calif.',\n",
       " 'California',\n",
       " 'calif.',\n",
       " 'if.',\n",
       " 'Xxxxx.',\n",
       " 'Calif',\n",
       " 'calif',\n",
       " 'lif',\n",
       " 'Ca',\n",
       " 'can',\n",
       " 'ca',\n",
       " \"'ve\",\n",
       " 'have',\n",
       " 'Can',\n",
       " 've',\n",
       " 'v',\n",
       " '’ve',\n",
       " '’xx',\n",
       " 'Co.',\n",
       " 'co.',\n",
       " 'Co',\n",
       " 'co',\n",
       " 'Colo.',\n",
       " 'Colorado',\n",
       " 'colo.',\n",
       " 'lo.',\n",
       " 'Colo',\n",
       " 'colo',\n",
       " 'olo',\n",
       " 'Conn.',\n",
       " 'Connecticut',\n",
       " 'conn.',\n",
       " 'nn.',\n",
       " 'Conn',\n",
       " 'conn',\n",
       " 'onn',\n",
       " 'Corp.',\n",
       " 'corp.',\n",
       " 'rp.',\n",
       " 'Corp',\n",
       " 'corp',\n",
       " 'orp',\n",
       " 'Could',\n",
       " 'could',\n",
       " 'uld',\n",
       " 'C’m',\n",
       " 'c’m',\n",
       " 'X’x',\n",
       " 'D.C.',\n",
       " 'd.c.',\n",
       " '.C.',\n",
       " 'X.X.',\n",
       " 'Dare',\n",
       " 'dare',\n",
       " 'Dec.',\n",
       " 'December',\n",
       " 'dec.',\n",
       " 'ec.',\n",
       " 'Dec',\n",
       " 'dec',\n",
       " 'Del.',\n",
       " 'Delaware',\n",
       " 'del.',\n",
       " 'el.',\n",
       " 'Del',\n",
       " 'del',\n",
       " 'Did',\n",
       " 'do',\n",
       " 'did',\n",
       " 'Does',\n",
       " 'does',\n",
       " 'oes',\n",
       " 'Doin',\n",
       " 'doing',\n",
       " 'doin',\n",
       " 'oin',\n",
       " \"Doin'\",\n",
       " \"doin'\",\n",
       " \"in'\",\n",
       " \"Xxxx'\",\n",
       " 'Doin’',\n",
       " 'doin’',\n",
       " 'in’',\n",
       " 'Xxxx’',\n",
       " 'Do',\n",
       " 'Dr.',\n",
       " 'dr.',\n",
       " 'Dr',\n",
       " 'dr',\n",
       " 'E.G.',\n",
       " 'e.g.',\n",
       " 'E',\n",
       " '.G.',\n",
       " 'E.g.',\n",
       " '.g.',\n",
       " 'X.x.',\n",
       " 'E.g',\n",
       " 'e.g',\n",
       " 'X.x',\n",
       " 'Feb.',\n",
       " 'February',\n",
       " 'feb.',\n",
       " 'F',\n",
       " 'eb.',\n",
       " 'Feb',\n",
       " 'feb',\n",
       " 'Fla.',\n",
       " 'Florida',\n",
       " 'fla.',\n",
       " 'Fla',\n",
       " 'fla',\n",
       " 'Ga.',\n",
       " 'Georgia',\n",
       " 'ga.',\n",
       " 'G',\n",
       " 'Ga',\n",
       " 'ga',\n",
       " 'Gen.',\n",
       " 'gen.',\n",
       " 'en.',\n",
       " 'Gen',\n",
       " 'gen',\n",
       " 'Goin',\n",
       " 'going',\n",
       " 'goin',\n",
       " \"Goin'\",\n",
       " \"goin'\",\n",
       " 'Goin’',\n",
       " 'goin’',\n",
       " 'Gon',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'to',\n",
       " 'Got',\n",
       " 'got',\n",
       " 'ta',\n",
       " 't',\n",
       " 'Gov.',\n",
       " 'gov.',\n",
       " 'ov.',\n",
       " 'Gov',\n",
       " 'gov',\n",
       " 'Had',\n",
       " 'had',\n",
       " 'H',\n",
       " 'Has',\n",
       " 'has',\n",
       " 'Have',\n",
       " 'ave',\n",
       " 'Havin',\n",
       " 'having',\n",
       " 'havin',\n",
       " 'vin',\n",
       " \"Havin'\",\n",
       " \"havin'\",\n",
       " \"Xxxxx'\",\n",
       " 'Havin’',\n",
       " 'havin’',\n",
       " 'Xxxxx’',\n",
       " 'He',\n",
       " 'he',\n",
       " 'would',\n",
       " \"He's\",\n",
       " \"he's\",\n",
       " \"e's\",\n",
       " \"Xx'x\",\n",
       " '’d',\n",
       " '’x',\n",
       " '’ll',\n",
       " '’s',\n",
       " 'He’s',\n",
       " 'he’s',\n",
       " 'e’s',\n",
       " 'Xx’x',\n",
       " 'How',\n",
       " 'how',\n",
       " \"'y\",\n",
       " 'you',\n",
       " \"How's\",\n",
       " \"how's\",\n",
       " \"w's\",\n",
       " \"Xxx'x\",\n",
       " '’y',\n",
       " '’re',\n",
       " 'How’s',\n",
       " 'how’s',\n",
       " 'w’s',\n",
       " 'Xxx’x',\n",
       " 'I',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'gonna',\n",
       " 'I.E.',\n",
       " 'i.e.',\n",
       " '.E.',\n",
       " 'I.e.',\n",
       " '.e.',\n",
       " 'I.e',\n",
       " 'i.e',\n",
       " 'Ia.',\n",
       " 'Iowa',\n",
       " 'ia.',\n",
       " 'Ia',\n",
       " 'ia',\n",
       " 'Id.',\n",
       " 'Idaho',\n",
       " 'id.',\n",
       " 'Id',\n",
       " 'id',\n",
       " 'Ill.',\n",
       " 'Illinois',\n",
       " 'ill.',\n",
       " 'll.',\n",
       " 'Ill',\n",
       " 'ill',\n",
       " 'm',\n",
       " 'Inc.',\n",
       " 'inc.',\n",
       " 'nc.',\n",
       " 'Inc',\n",
       " 'inc',\n",
       " 'Ind.',\n",
       " 'Indiana',\n",
       " 'ind.',\n",
       " 'nd.',\n",
       " 'Ind',\n",
       " 'ind',\n",
       " 'Is',\n",
       " 'is',\n",
       " 'It',\n",
       " 'it',\n",
       " \"It's\",\n",
       " \"it's\",\n",
       " \"t's\",\n",
       " 'It’s',\n",
       " 'it’s',\n",
       " 't’s',\n",
       " '’m',\n",
       " 'Jan.',\n",
       " 'January',\n",
       " 'jan.',\n",
       " 'J',\n",
       " 'an.',\n",
       " 'Jan',\n",
       " 'jan',\n",
       " 'Jr.',\n",
       " 'jr.',\n",
       " 'Jr',\n",
       " 'jr',\n",
       " 'Jul.',\n",
       " 'July',\n",
       " 'jul.',\n",
       " 'ul.',\n",
       " 'Jul',\n",
       " 'jul',\n",
       " 'Jun.',\n",
       " 'June',\n",
       " 'jun.',\n",
       " 'un.',\n",
       " 'Jun',\n",
       " 'jun',\n",
       " 'Kan.',\n",
       " 'Kansas',\n",
       " 'kan.',\n",
       " 'K',\n",
       " 'Kan',\n",
       " 'kan',\n",
       " 'Kans.',\n",
       " 'kans.',\n",
       " 'ns.',\n",
       " 'Kans',\n",
       " 'kans',\n",
       " 'ans',\n",
       " 'Ky.',\n",
       " 'Kentucky',\n",
       " 'ky.',\n",
       " 'Ky',\n",
       " 'ky',\n",
       " 'La.',\n",
       " 'Louisiana',\n",
       " 'L',\n",
       " 'La',\n",
       " 'la',\n",
       " 'Let',\n",
       " 'let',\n",
       " 'us',\n",
       " \"Let's\",\n",
       " \"let's\",\n",
       " 'Let’s',\n",
       " 'let’s',\n",
       " 'Lovin',\n",
       " 'loving',\n",
       " 'lovin',\n",
       " \"Lovin'\",\n",
       " \"lovin'\",\n",
       " 'Lovin’',\n",
       " 'lovin’',\n",
       " 'Ltd.',\n",
       " 'ltd.',\n",
       " 'td.',\n",
       " 'Ltd',\n",
       " 'ltd',\n",
       " \"Ma'am\",\n",
       " 'madam',\n",
       " \"ma'am\",\n",
       " 'M',\n",
       " \"'am\",\n",
       " \"Xx'xx\",\n",
       " 'Mar.',\n",
       " 'March',\n",
       " 'mar.',\n",
       " 'ar.',\n",
       " 'Mar',\n",
       " 'mar',\n",
       " 'Mass.',\n",
       " 'Massachusetts',\n",
       " 'mass.',\n",
       " 'ss.',\n",
       " 'Mass',\n",
       " 'mass',\n",
       " 'ass',\n",
       " 'May.',\n",
       " 'May',\n",
       " 'may.',\n",
       " 'ay.',\n",
       " 'may',\n",
       " 'Ma’am',\n",
       " 'ma’am',\n",
       " '’am',\n",
       " 'Xx’xx',\n",
       " 'Md.',\n",
       " 'md.',\n",
       " 'Md',\n",
       " 'md',\n",
       " 'Messrs.',\n",
       " 'messrs.',\n",
       " 'rs.',\n",
       " 'Messrs',\n",
       " 'messrs',\n",
       " 'srs',\n",
       " 'Mich.',\n",
       " 'Michigan',\n",
       " 'mich.',\n",
       " 'ch.',\n",
       " 'Mich',\n",
       " 'mich',\n",
       " 'ich',\n",
       " 'Might',\n",
       " 'might',\n",
       " 'ght',\n",
       " 'Minn.',\n",
       " 'Minnesota',\n",
       " 'minn.',\n",
       " 'Minn',\n",
       " 'minn',\n",
       " 'inn',\n",
       " 'Miss.',\n",
       " 'Mississippi',\n",
       " 'miss.',\n",
       " 'Miss',\n",
       " 'miss',\n",
       " 'iss',\n",
       " 'Mo.',\n",
       " 'mo.',\n",
       " 'Mo',\n",
       " 'mo',\n",
       " 'Mont.',\n",
       " 'mont.',\n",
       " 'nt.',\n",
       " 'Mont',\n",
       " 'mont',\n",
       " 'ont',\n",
       " 'Mr.',\n",
       " 'mr.',\n",
       " 'Mr',\n",
       " 'mr',\n",
       " 'Mrs.',\n",
       " 'mrs.',\n",
       " 'Mrs',\n",
       " 'mrs',\n",
       " 'Ms.',\n",
       " 'ms.',\n",
       " 'Ms',\n",
       " 'ms',\n",
       " 'Mt.',\n",
       " 'Mount',\n",
       " 'mt.',\n",
       " 'Mt',\n",
       " 'mt',\n",
       " 'Must',\n",
       " 'must',\n",
       " 'ust',\n",
       " 'N.C.',\n",
       " 'North Carolina',\n",
       " 'n.c.',\n",
       " 'N',\n",
       " 'N.D.',\n",
       " 'North Dakota',\n",
       " 'n.d.',\n",
       " '.D.',\n",
       " 'N.H.',\n",
       " 'New Hampshire',\n",
       " 'n.h.',\n",
       " '.H.',\n",
       " 'N.J.',\n",
       " 'New Jersey',\n",
       " 'n.j.',\n",
       " '.J.',\n",
       " 'N.M.',\n",
       " 'New Mexico',\n",
       " 'n.m.',\n",
       " '.M.',\n",
       " 'N.Y.',\n",
       " 'New York',\n",
       " 'n.y.',\n",
       " '.Y.',\n",
       " 'Neb.',\n",
       " 'Nebraska',\n",
       " 'neb.',\n",
       " 'Neb',\n",
       " 'neb',\n",
       " 'Nebr.',\n",
       " 'nebr.',\n",
       " 'br.',\n",
       " 'Nebr',\n",
       " 'nebr',\n",
       " 'ebr',\n",
       " 'Need',\n",
       " 'need',\n",
       " 'eed',\n",
       " 'Nev.',\n",
       " 'Nevada',\n",
       " 'nev.',\n",
       " 'ev.',\n",
       " 'Nev',\n",
       " 'nev',\n",
       " 'Not',\n",
       " 'Nothin',\n",
       " 'nothing',\n",
       " 'nothin',\n",
       " 'hin',\n",
       " \"Nothin'\",\n",
       " \"nothin'\",\n",
       " 'Nothin’',\n",
       " 'nothin’',\n",
       " 'Nov.',\n",
       " 'November',\n",
       " 'nov.',\n",
       " 'Nov',\n",
       " 'nov',\n",
       " 'Nuthin',\n",
       " 'nuthin',\n",
       " \"Nuthin'\",\n",
       " \"nuthin'\",\n",
       " 'Nuthin’',\n",
       " 'nuthin’',\n",
       " \"O'clock\",\n",
       " \"o'clock\",\n",
       " 'ock',\n",
       " \"X'xxxx\",\n",
       " 'O.O',\n",
       " 'o.o',\n",
       " 'X.X',\n",
       " 'O.o',\n",
       " 'O_O',\n",
       " 'o_o',\n",
       " 'X_X',\n",
       " 'O_o',\n",
       " 'X_x',\n",
       " 'Oct.',\n",
       " 'October',\n",
       " 'oct.',\n",
       " 'ct.',\n",
       " 'Oct',\n",
       " 'oct',\n",
       " 'Okla.',\n",
       " 'Oklahoma',\n",
       " 'okla.',\n",
       " 'Okla',\n",
       " 'okla',\n",
       " 'kla',\n",
       " 'Ol',\n",
       " 'old',\n",
       " 'ol',\n",
       " \"Ol'\",\n",
       " \"ol'\",\n",
       " \"Xx'\",\n",
       " 'Ol’',\n",
       " 'ol’',\n",
       " 'Xx’',\n",
       " 'Ore.',\n",
       " 'Oregon',\n",
       " 'ore.',\n",
       " 're.',\n",
       " 'Ore',\n",
       " 'ore',\n",
       " 'Ought',\n",
       " 'ought',\n",
       " 'O’clock',\n",
       " 'o’clock',\n",
       " 'X’xxxx',\n",
       " 'Pa.',\n",
       " 'Pennsylvania',\n",
       " 'pa.',\n",
       " 'Pa',\n",
       " 'pa',\n",
       " 'Ph.D.',\n",
       " 'ph.d.',\n",
       " 'Xx.X.',\n",
       " 'Ph',\n",
       " 'ph',\n",
       " 'D.',\n",
       " 'd.',\n",
       " 'X.',\n",
       " 'Prof.',\n",
       " 'prof.',\n",
       " 'of.',\n",
       " 'Prof',\n",
       " 'prof',\n",
       " 'rof',\n",
       " 'Rep.',\n",
       " 'rep.',\n",
       " 'R',\n",
       " 'ep.',\n",
       " 'Rep',\n",
       " 'rep',\n",
       " 'Rev.',\n",
       " 'rev.',\n",
       " 'Rev',\n",
       " 'rev',\n",
       " 'S.C.',\n",
       " 'South Carolina',\n",
       " 's.c.',\n",
       " 'Sen.',\n",
       " 'sen.',\n",
       " 'Sen',\n",
       " 'sen',\n",
       " 'Sep.',\n",
       " 'September',\n",
       " 'sep.',\n",
       " 'Sep',\n",
       " 'sep',\n",
       " 'Sept.',\n",
       " 'sept.',\n",
       " 'pt.',\n",
       " 'Sept',\n",
       " 'sept',\n",
       " 'ept',\n",
       " 'Sha',\n",
       " 'shall',\n",
       " 'sha',\n",
       " 'She',\n",
       " 'she',\n",
       " \"She's\",\n",
       " \"she's\",\n",
       " 'She’s',\n",
       " 'she’s',\n",
       " 'Should',\n",
       " 'should',\n",
       " 'Somethin',\n",
       " 'something',\n",
       " 'somethin',\n",
       " \"Somethin'\",\n",
       " \"somethin'\",\n",
       " 'Somethin’',\n",
       " 'somethin’',\n",
       " 'St.',\n",
       " 'st.',\n",
       " 'St',\n",
       " 'st',\n",
       " 'Tenn.',\n",
       " 'Tennessee',\n",
       " 'tenn.',\n",
       " 'T',\n",
       " 'Tenn',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>terrible place to work for i just heard a stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>hours , minutes total time for an extremely s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>my less than stellar review is for service . w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>i m granting one star because there s no way t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>the food here is mediocre at best . i went aft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                                             review\n",
       "0  negative  terrible place to work for i just heard a stor...\n",
       "1  negative   hours , minutes total time for an extremely s...\n",
       "2  negative  my less than stellar review is for service . w...\n",
       "3  negative  i m granting one star because there s no way t...\n",
       "4  negative  the food here is mediocre at best . i went aft..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55995</th>\n",
       "      <td>positive</td>\n",
       "      <td>great food . wonderful , friendly service . i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55996</th>\n",
       "      <td>positive</td>\n",
       "      <td>charlotte should be the new standard for moder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55997</th>\n",
       "      <td>positive</td>\n",
       "      <td>get the encore sandwich ! ! make sure to get i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55998</th>\n",
       "      <td>positive</td>\n",
       "      <td>i m a pretty big ice cream gelato fan . pretty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55999</th>\n",
       "      <td>positive</td>\n",
       "      <td>where else can you find all the parts and piec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rating                                             review\n",
       "55995  positive  great food . wonderful , friendly service . i ...\n",
       "55996  positive  charlotte should be the new standard for moder...\n",
       "55997  positive  get the encore sandwich ! ! make sure to get i...\n",
       "55998  positive  i m a pretty big ice cream gelato fan . pretty...\n",
       "55999  positive  where else can you find all the parts and piec..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeVocabulary():\n",
    "    unkToken = '<UNK>'\n",
    "    vocab['t_2_i'] = {}\n",
    "    vocab['i_2_t'] = {}\n",
    "    vocab['unkToken'] = unkToken\n",
    "    idx = addToken(unkToken)\n",
    "    vocab['unkTokenIdx'] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToken(token):\n",
    "    if token in vocab['t_2_i']:\n",
    "        idx = vocab['t_2_i'][token]\n",
    "    else:\n",
    "        idx = len(vocab['t_2_i'])\n",
    "        vocab['t_2_i'][token] = idx\n",
    "        vocab['i_2_t'][idx] = token\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addManyTokens(tokens):\n",
    "    idxes = [addToken(token) for token in tokens]\n",
    "    return idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookUpToken(token):\n",
    "    if vocab['unkTokenIdx']>=0:\n",
    "        return vocab['t_2_i'].get(token,vocab['unkTokenIdx'])\n",
    "    else:\n",
    "        return vocab['t_2_i'][token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookUpIndex(idx):\n",
    "    if idx not in vocab['i_2_t']:\n",
    "        raise KeyError(\"the index (%d) is not there\" % idx)\n",
    "    return vocab['i_2_t'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabularyFromDataFrame(df,cutoff=25):\n",
    "    initializeVocabulary()\n",
    "    wordCounts = Counter()\n",
    "    for r in df.review:\n",
    "        for word in re.split('\\W+',r):\n",
    "            wordCounts[word] += 1\n",
    "    for word,count in wordCounts.items():\n",
    "        if count > cutoff:\n",
    "            addToken(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabularyFromCorpus(Corpus,cutoff=25):\n",
    "    initializeVocabulary()\n",
    "    wordCounts = Counter()\n",
    "    for doc in Corpus:\n",
    "        for word in re.split('\\W+',doc):\n",
    "            wordCounts[word] += 1\n",
    "    for word,count in wordCounts.items():\n",
    "        if count > cutoff:\n",
    "            addToken(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabularyFromDataFrame(df)\n",
    "Corpus = np.asarray(df['review'])\n",
    "vocabularyFromCorpus(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookUpToken('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookUpIndex(38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8946"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab['t_2_i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotVector(token,N):\n",
    "    oneHot = np.zeros((N,1))\n",
    "    oneHot[lookUpToken(token)] = 1\n",
    "    return oneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(vocab['t_2_i'])\n",
    "token = 'the'\n",
    "oneHot = oneHotVector(token,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneHot[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFeatures(doc,N):\n",
    "    isFirst = True\n",
    "    for token in doc:\n",
    "        oneHot = oneHotVector(token,N)\n",
    "        if isFirst:\n",
    "            xF = oneHot\n",
    "            isFirst = False\n",
    "        else:\n",
    "            xF = np.hstack((xF,oneHot))\n",
    "    return np.mean(xF,axis=1)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFeatures_fast(doc,N):\n",
    "    fv = np.zeros(N)\n",
    "    numTokens = 0\n",
    "    for token in doc:\n",
    "        fv[lookUpToken(token)] += 1\n",
    "        numTokens += 1\n",
    "    return fv/numTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpusToFeatureMatrix(Corpus,N):\n",
    "    isFirst = True\n",
    "    for doc in Corpus:\n",
    "        fv = computeFeatures(doc,N)\n",
    "        if isFirst:\n",
    "            fM = fv\n",
    "            isFirst = False\n",
    "        else:\n",
    "            fM = np.hstack((fM,fv))\n",
    "    return fM.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpusToFeatureMatrix_fast(Corpus,N):\n",
    "    fM = np.zeros((N,len(Corpus)))\n",
    "    i = 0\n",
    "    for doc in Corpus:\n",
    "        fM[:,i] = computeFeatures_fast(doc,N)\n",
    "        i+=1\n",
    "    return fM.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682 µs ± 56.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fv = computeFeatures_fast(Corpus[0],len(vocab['t_2_i']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.2 s ± 2.45 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fv = computeFeatures(Corpus[0],len(vocab['t_2_i']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviews.csv')\n",
    "X = np.asarray(df['review'])\n",
    "y = np.asarray(df['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=0.3,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularyFromCorpus(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(vocab['t_2_i'])\n",
    "Xtrain_fM = corpusToFeatureMatrix_fast(Xtrain,N)\n",
    "Xtest_fM = corpusToFeatureMatrix_fast(Xtest,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39200, 7304)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_fM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16800, 7304)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_fM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LogisticRegression as clf\n",
    "#from sklearn.naive_bayes import GaussianNB as clf\n",
    "#from sklearn.ensemble import RandomForestClassifier as clf\n",
    "from sklearn.svm import SVC as clf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = clf().fit(Xtrain_fM,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = M.predict(Xtest_fM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(ytest,y_pred)\n",
    "sns.heatmap(mat.T,square=True,annot=True,fmt='d',cbar=False,\n",
    "           xticklabels=np.unique(y),yticklabels=np.unique(y))\n",
    "plt.xlabel(\"True Label\")\n",
    "plt.ylabel(\"Predicted Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39200,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
