{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputString = [2,45,30,55,10]\n",
    "outputString = [45,30,55,10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeatures = 100\n",
    "vocabSize = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for i in range(len(inputString)):\n",
    "    x = np.random.randn(numFeatures,1)\n",
    "    embeddings.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOneHot(idx):\n",
    "    one_hot = np.zeros((vocabSize,1))\n",
    "    one_hot[idx] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(getOneHot(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numUnits = 50\n",
    "h0 = torch.tensor(np.zeros((numUnits,1)))\n",
    "Wh = torch.tensor(np.random.uniform(0,1,(numUnits,numUnits)),requires_grad=True)\n",
    "Wx = torch.tensor(np.random.uniform(0,1,(numUnits,numFeatures)),requires_grad=True)\n",
    "Wy = torch.tensor(np.random.uniform(0,1,(vocabSize,numUnits)),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 50]) torch.Size([50, 100]) torch.Size([80, 50]) torch.Size([50, 1])\n"
     ]
    }
   ],
   "source": [
    "print(Wh.shape,Wx.shape,Wy.shape,h0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepForward(xt,Wx,Wh,Wy,prevMemory):\n",
    "    x_frd = torch.matmul(Wx,torch.from_numpy(xt))\n",
    "    h_frd = torch.matmul(Wh,prevMemory)\n",
    "    ht = torch.tanh(x_frd+h_frd)\n",
    "    yt_hat = F.softmax(torch.matmul(Wy,ht),dim=0)\n",
    "    return ht,yt_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht,yt_hat = stepForward(embeddings[0],Wx,Wh,Wy,h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., dtype=torch.float64, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_hat.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullForwardRNN(X,Wx,Wh,Wy,prevMemory):\n",
    "    y_hat = []\n",
    "    for t in range(len(X)):\n",
    "        ht,yt_hat = stepForward(X[t],Wx,Wh,Wy,prevMemory)\n",
    "        prevMemory = ht\n",
    "        y_hat.append(yt_hat)\n",
    "    return y_hat  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = fullForwardRNN(embeddings,Wx,Wh,Wy,h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLoss(y,y_hat):\n",
    "    loss = 0\n",
    "    for yi,yi_hat in zip(y,y_hat):\n",
    "        Li = -torch.log2(yi_hat[yi==1])\n",
    "        loss += Li\n",
    "    return loss/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for idx in outputString:\n",
    "    y.append(getOneHot(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.6703], dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(computeLoss(y,y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParams(Wx,Wh,Wy,dWx,dWh,dWy,lr):\n",
    "    with torch.no_grad():\n",
    "        Wx -= lr*dWx\n",
    "        Wh -= lr*dWh\n",
    "        Wy -= lr*dWy\n",
    "    return Wx,Wh,Wy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRNN(X,y,Wx,Wh,Wy,prevMemory,lr,nepoch):\n",
    "    losses = []\n",
    "    for epoch in range(nepoch):\n",
    "        y_hat = fullForwardRNN(X,Wx,Wh,Wy,prevMemory)\n",
    "        loss = computeLoss(y,y_hat)\n",
    "        loss.backward()\n",
    "        losses.append(loss)\n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,loss))\n",
    "        sys.stdout.flush()\n",
    "        dWx = Wx.grad.data\n",
    "        dWh = Wh.grad.data\n",
    "        dWy = Wy.grad.data\n",
    "        Wx,Wh,Wy = updateParams(Wx,Wh,Wy,dWx,dWh,dWy,lr)\n",
    "        Wx.grad.data.zero_()\n",
    "        Wh.grad.data.zero_()\n",
    "        Wy.grad.data.zero_()\n",
    "    return Wx,Wh,Wy,losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch=0: 8.670349\n",
      "Loss after epoch=1: 8.626533\n",
      "Loss after epoch=2: 8.582421\n",
      "Loss after epoch=3: 8.538054\n",
      "Loss after epoch=4: 8.493490\n",
      "Loss after epoch=5: 8.448793\n",
      "Loss after epoch=6: 8.404036\n",
      "Loss after epoch=7: 8.359290\n",
      "Loss after epoch=8: 8.314623\n",
      "Loss after epoch=9: 8.270099\n",
      "Loss after epoch=10: 8.225770\n",
      "Loss after epoch=11: 8.181687\n",
      "Loss after epoch=12: 8.137891\n",
      "Loss after epoch=13: 8.094423\n",
      "Loss after epoch=14: 8.051320\n",
      "Loss after epoch=15: 8.008617\n",
      "Loss after epoch=16: 7.966346\n",
      "Loss after epoch=17: 7.924532\n",
      "Loss after epoch=18: 7.883191\n",
      "Loss after epoch=19: 7.842329\n",
      "Loss after epoch=20: 7.801939\n",
      "Loss after epoch=21: 7.762001\n",
      "Loss after epoch=22: 7.722486\n",
      "Loss after epoch=23: 7.683352\n",
      "Loss after epoch=24: 7.644553\n",
      "Loss after epoch=25: 7.606033\n",
      "Loss after epoch=26: 7.567735\n",
      "Loss after epoch=27: 7.529593\n",
      "Loss after epoch=28: 7.491539\n",
      "Loss after epoch=29: 7.453496\n",
      "Loss after epoch=30: 7.415383\n",
      "Loss after epoch=31: 7.377109\n",
      "Loss after epoch=32: 7.338572\n",
      "Loss after epoch=33: 7.299660\n",
      "Loss after epoch=34: 7.260244\n",
      "Loss after epoch=35: 7.220181\n",
      "Loss after epoch=36: 7.179315\n",
      "Loss after epoch=37: 7.137476\n",
      "Loss after epoch=38: 7.094490\n",
      "Loss after epoch=39: 7.050189\n",
      "Loss after epoch=40: 7.004436\n",
      "Loss after epoch=41: 6.957151\n",
      "Loss after epoch=42: 6.908346\n",
      "Loss after epoch=43: 6.858159\n",
      "Loss after epoch=44: 6.806877\n",
      "Loss after epoch=45: 6.754924\n",
      "Loss after epoch=46: 6.702824\n",
      "Loss after epoch=47: 6.651121\n",
      "Loss after epoch=48: 6.600298\n",
      "Loss after epoch=49: 6.550708\n",
      "Loss after epoch=50: 6.502546\n",
      "Loss after epoch=51: 6.455847\n",
      "Loss after epoch=52: 6.410521\n",
      "Loss after epoch=53: 6.366375\n",
      "Loss after epoch=54: 6.323133\n",
      "Loss after epoch=55: 6.280420\n",
      "Loss after epoch=56: 6.237684\n",
      "Loss after epoch=57: 6.193996\n",
      "Loss after epoch=58: 6.147388\n",
      "Loss after epoch=59: 6.092322\n",
      "Loss after epoch=60: 6.005315\n",
      "Loss after epoch=61: 5.761695\n",
      "Loss after epoch=62: 5.578651\n",
      "Loss after epoch=63: 5.540952\n",
      "Loss after epoch=64: 5.505168\n",
      "Loss after epoch=65: 5.470372\n",
      "Loss after epoch=66: 5.435828\n",
      "Loss after epoch=67: 5.401421\n",
      "Loss after epoch=68: 5.367153\n",
      "Loss after epoch=69: 5.333033\n",
      "Loss after epoch=70: 5.299071\n",
      "Loss after epoch=71: 5.265277\n",
      "Loss after epoch=72: 5.231664\n",
      "Loss after epoch=73: 5.198242\n",
      "Loss after epoch=74: 5.165024\n",
      "Loss after epoch=75: 5.132020\n",
      "Loss after epoch=76: 5.099242\n",
      "Loss after epoch=77: 5.066700\n",
      "Loss after epoch=78: 5.034403\n",
      "Loss after epoch=79: 5.002361\n",
      "Loss after epoch=80: 4.970581\n",
      "Loss after epoch=81: 4.939068\n",
      "Loss after epoch=82: 4.907827\n",
      "Loss after epoch=83: 4.876862\n",
      "Loss after epoch=84: 4.846174\n",
      "Loss after epoch=85: 4.815763\n",
      "Loss after epoch=86: 4.785628\n",
      "Loss after epoch=87: 4.755764\n",
      "Loss after epoch=88: 4.726170\n",
      "Loss after epoch=89: 4.696839\n",
      "Loss after epoch=90: 4.667768\n",
      "Loss after epoch=91: 4.638950\n",
      "Loss after epoch=92: 4.610383\n",
      "Loss after epoch=93: 4.582067\n",
      "Loss after epoch=94: 4.554002\n",
      "Loss after epoch=95: 4.526195\n",
      "Loss after epoch=96: 4.498657\n",
      "Loss after epoch=97: 4.471403\n",
      "Loss after epoch=98: 4.444451\n",
      "Loss after epoch=99: 4.417821\n"
     ]
    }
   ],
   "source": [
    "Wx,Wh,Wy,losses = trainRNN(embeddings,y,Wx,Wh,Wy,h0,0.001,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
